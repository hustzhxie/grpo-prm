import time
from abc import ABC
from copy import deepcopy
from dataclasses import dataclass, fields
from datetime import timedelta
from typing import Any, List, Tuple, Union

from numpy import append

import ray
import torch
import time

from openrlhf.models.utils import compute_approx_kl, compute_reward, masked_mean
from openrlhf.trainer.ray.launcher import RayActorGroup
from openrlhf.utils.logging_utils import init_logger
from openrlhf.utils.seqlen_balancing import get_minimum_num_micro_batch_size, get_seqlen_balanced_partitions
from openrlhf.utils.utils import remove_pad_token, zero_pad_sequences

logger = init_logger(__name__)


def to(tensor: Union[torch.Tensor, list[torch.Tensor]], device):
    if isinstance(tensor, list):
        return [to(t, device) for t in tensor]
    return tensor.to(device) if isinstance(tensor, torch.Tensor) else tensor


def pin_memory(tensor: Union[torch.Tensor, list[torch.Tensor]]):
    if isinstance(tensor, list):
        return [pin_memory(t) for t in tensor]
    return tensor.pin_memory() if isinstance(tensor, torch.Tensor) else tensor


@dataclass
class Experience:
    """Experience is a batch of data for RLHF training.

    Shapes of each tensor:
    index: (B,)
    sequences: (B, S)
    attention_mask: (B, S)
    action_mask: (B, A)
    action_log_probs: (B, S)
    base_action_log_probs: (B, S)
    values: (B, S)
    returns: (B, S)
    advantages: (B, S)
    kl: (B, S)
    info: dict[str, list]
    """

    index: list[int] = None
    sequences: torch.Tensor = None
    attention_mask: torch.LongTensor = None
    action_mask: torch.BoolTensor = None

    action_log_probs: torch.Tensor = None
    base_action_log_probs: torch.Tensor = None
    rollout_log_probs: torch.Tensor = None
    values: torch.Tensor = None
    returns: torch.Tensor = None
    advantages: torch.Tensor = None
    kl: torch.Tensor = None

    prompts: list[str] = None
    labels: list[str] = None
    rewards: torch.Tensor = None  # used for advantage calculation
    scores: torch.Tensor = None  # 0-1 reward used for dynamic sampling

    # the info field is used to store additional information
    # all the fields in the info will be logged to the tensorboard/wandb
    info: dict[str, torch.Tensor] = None

    def __init__(
        self,
        index=None,
        sequences=None,
        action_log_probs=None,
        base_action_log_probs=None,
        rollout_log_probs=None,
        values=None,
        returns=None,
        advantages=None,
        attention_mask=None,
        action_mask=None,
        kl=None,
        prompts=None,
        labels=None,
        rewards=None,
        scores=None,
        info=None,
    ):
        self.index = index
        self.sequences = sequences
        self.action_log_probs = action_log_probs
        self.base_action_log_probs = base_action_log_probs
        self.rollout_log_probs = rollout_log_probs
        self.values = values
        self.returns = returns
        self.advantages = advantages
        self.attention_mask = attention_mask
        self.action_mask = action_mask
        self.kl = kl
        self.prompts = prompts or []
        self.labels = labels or []
        self.rewards = rewards
        self.scores = scores
        self.info = info or []

    @torch.no_grad()
    def to_device(self, device: torch.device):
        """Move all tensor fields to the specified device."""
        for field, value in self.__dict__.items():
            if isinstance(value, dict):
                setattr(self, field, {key: to(val, device) for key, val in value.items()})
            else:
                setattr(self, field, to(value, device))

        return self

    def pin_memory(self):
        """Pin memory for all tensor fields."""
        for field, value in self.__dict__.items():
            if isinstance(value, dict):
                setattr(self, field, {key: pin_memory(val) for key, val in value.items()})
            else:
                setattr(self, field, pin_memory(value))

        return self

    @staticmethod
    def select(experiences: List["Experience"], fields: List[str]) -> List["Experience"]:
        """Select specific fields from a list of Experience instances to create new Experience instances.

        Args:
            experiences: List of Experience instances
            fields: List of field names to select

        Returns:
            A list of new Experience instances containing only the selected fields
        """
        new_experiences = []
        for exp in experiences:
            new_exp = Experience()
            for field in fields:
                if hasattr(exp, field):
                    setattr(new_exp, field, getattr(exp, field))
            new_experiences.append(new_exp)
        return new_experiences

    @staticmethod
    def _merge_item(items: List, pad_value: int = 0) -> Union[torch.Tensor, list, dict, Any]:
        """Merge a list of items into a single item.
        Recursively merge tensors, lists and dicts.
        For tensors, use zero_pad_sequences to merge sequences of different lengths.

        Args:
            items: List of items to merge
            pad_value: Value used for padding tensors
        """
        if isinstance(items[0], torch.Tensor):
            return zero_pad_sequences(items, side="right", value=pad_value)
        elif isinstance(items[0], list):
            return sum(items, [])
        elif isinstance(items[0], dict):
            result = {}
            # Collect all values for each key
            for d in items:
                for key, value in d.items():
                    if key not in result:
                        result[key] = []
                    result[key].append(value)
            # Merge all values for each key at once
            return {key: Experience._merge_item(values, pad_value) for key, values in result.items()}
        elif items[0] is None:
            return None
        else:
            raise ValueError(f"Unsupported type: {type(items[0])}")

    @staticmethod
    def concat_experiences(experiences_list: List["Experience"], pad_token_id) -> "Experience":
        """Concatenate multiple experiences into one large experience.

        Args:
            experiences_list: List of Experience to concatenate
            pad_token_id: Token id used for padding sequences

        Returns:
            A new Experience instance containing all the concatenated data
        """
        if not experiences_list:
            return Experience()

        # Get all field names from the dataclass
        field_names = [f.name for f in fields(Experience)]

        # Create result dictionary
        result = {}

        # Merge all fields
        for field in field_names:
            values = [getattr(e, field) for e in experiences_list]
            # Use pad_token_id for sequences field, 0 for others
            pad_value = pad_token_id if field == "sequences" else 0
            result[field] = Experience._merge_item(values, pad_value)

        return Experience(**result)


def update_samples_with_rewards(rewards_info, samples_list):
    """Process rewards info and update samples with rewards, scores and extra logs.

    Args:
        rewards_info: List of reward information dictionaries
        samples_list: List of Experience objects to update
    """
    # Process rewards and scores
    samples_len = [len(sample.sequences) for sample in samples_list]   # èŽ·å–æ¯ä¸ªexperienceä¸­çš„æ ·æœ¬æ•°é‡
    rewards_list = torch.cat([info["rewards"] for info in rewards_info], dim=0).split(samples_len)
    if "scores" in rewards_info[0]:
        scores_list = torch.cat([info["scores"] for info in rewards_info], dim=0).split(samples_len)
    else:
        scores_list = rewards_list

    # Process extra_logs if present
    if "extra_logs" in rewards_info[0]:
        # Merge all extra_logs tensors first
        merged_logs = {
            key: torch.cat([logs[key] for logs in [info["extra_logs"] for info in rewards_info]], dim=0).split(
                samples_len
            )
            for key in rewards_info[0]["extra_logs"].keys()
        }

    # Update samples with rewards, scores and extra logs
    for i, samples in enumerate(samples_list):
        samples.rewards = rewards_list[i]  # æ—¢å¯èƒ½æ˜¯token-levelçš„å¥–åŠ±ï¼Œä¹Ÿå¯èƒ½æ˜¯sample-levelçš„å¥–åŠ±
        samples.scores = scores_list[i]
        samples.info["score"] = scores_list[i]
        samples.info["reward"] = rewards_list[i]
        if "extra_logs" in rewards_info[0]:
            for key, values in merged_logs.items():
                samples.info[key] = values[i]

    return samples_list


class SamplesGenerator:
    def __init__(self, vllm_engines, strategy, tokenizer, prompt_max_len):
        self.strategy = strategy
        self.args = strategy.args
        self.vllm_engines = vllm_engines
        self.tokenizer = tokenizer
        self.prompt_max_len = prompt_max_len

    @torch.no_grad()
    def generate_samples(self, all_prompts: List[str], all_labels, **generate_kwargs) -> List[Experience]:
        """
        Generate samples and return in batches.

        When not using vllm, we will fallback to the default implementation,
        in which actor will be used to generate samples.
        """
        # vLLM wakeup when vllm_enable_sleep
        if self.strategy.args.vllm_enable_sleep:
            from openrlhf.trainer.ray.vllm_engine import batch_vllm_engine_call

            batch_vllm_engine_call(self.vllm_engines, "wake_up")

        rollout_samples = self._generate_vllm(all_prompts, all_labels, **generate_kwargs)

        # vLLM offload when vllm_enable_sleep
        if self.strategy.args.vllm_enable_sleep:
            batch_vllm_engine_call(self.vllm_engines, "sleep")

        return rollout_samples

        # tokenizer

    def tokenize_fn(self, texts, max_length, padding=True, device=None):
        if not padding:
            # when padding is False, return tokenized texts as list
            return self.tokenizer(
                texts,
                add_special_tokens=False,
                max_length=max_length,
                truncation=True,
            )
        batch = self.tokenizer(
            texts,
            return_tensors="pt",
            add_special_tokens=False,
            max_length=max_length,
            padding=True,
            truncation=True,
        )
        return {k: v.to(device) for k, v in batch.items()}

    def _generate_vllm(self, all_prompts: List[str], all_labels, **kwargs) -> List[Experience]:
        """Generate samples using vLLM engine.

        Args:
            all_prompts: List of prompts to generate from
            all_labels: List of labels corresponding to prompts
            **kwargs: Additional arguments for generation

        Returns:
            List of Experience objects containing generated samples
        """
        from vllm import SamplingParams
        # è¯´æ˜Žï¼šSamplingParams æ˜¯ vLLM çš„é‡‡æ ·é…ç½®å¯¹è±¡ï¼Œç”¨äºŽæŒ‡å®šè§£ç æ—¶çš„é‡‡æ ·/æˆªæ–­ç­–ç•¥
        # åŒ…æ‹¬æ¸©åº¦ã€top-pã€top-kã€æœ€å¤§/æœ€å°ç”Ÿæˆé•¿åº¦ã€æ˜¯å¦è·³è¿‡ç‰¹æ®Šç¬¦å·ã€æ˜¯å¦æŠŠåœæ­¢è¯åŒ…å«è¿›è¾“å‡ºç­‰ã€‚

        llms = self.vllm_engines
        args = self.strategy.args
        # llmsï¼šä¸€ç»„ vLLM å¼•æ“Žï¼ˆé€šå¸¸æ˜¯ Ray Actor å°è£…çš„è¿œç«¯æŽ¨ç†æœåŠ¡ï¼‰
        # argsï¼šè®­ç»ƒ/é‡‡æ ·çš„å…¨å±€å‚æ•°å¯¹è±¡ï¼Œä»Ž strategy ä¸­ç»§æ‰¿

        # Set up sampling parameters
        sampling_params = SamplingParams(
            temperature=kwargs.get("temperature", 1.0),
            top_p=kwargs.get("top_p", 1.0),
            top_k=kwargs.get("top_k", -1),
            max_tokens=kwargs.get("max_new_tokens", 1024),
            min_tokens=kwargs.get("min_new_tokens", 1),
            skip_special_tokens=kwargs.get("skip_special_tokens", False),
            include_stop_str_in_output=True,
            logprobs=1 if self.strategy.args.enable_vllm_is_correction else None,
        )
        # å…³é”®å‚æ•°é‡Šä¹‰ï¼š
        # - temperatureï¼šæ¸©åº¦é‡‡æ ·ï¼Œè¶Šå°è¶Šä¿å®ˆï¼Œè¶Šå¤§è¶Šå‘æ•£
        # - top_p / top_kï¼šæ ¸é‡‡æ ·/Top-K é‡‡æ ·çš„æˆªæ–­é˜ˆå€¼
        # - max_tokens / min_tokensï¼šæŽ§åˆ¶ç”Ÿæˆå›žå¤çš„ token ä¸Šé™/ä¸‹é™
        # - skip_special_tokensï¼šæ˜¯å¦åœ¨è§£ç æ–‡æœ¬æ—¶ä¸¢å¼ƒç‰¹æ®Š tokenï¼ˆå¦‚ EOSï¼‰
        # - include_stop_str_in_outputï¼šå‘½ä¸­åœæ­¢å­—ç¬¦ä¸²æ—¶æ˜¯å¦ä¿ç•™åˆ°è¾“å‡ºé‡Œ
        # - logprobsï¼šå½“å¯ç”¨ vLLM IS-correction æ—¶ï¼Œè¯·æ±‚è¿”å›žæ¯ä¸ªç”Ÿæˆ token çš„å¯¹æ•°æ¦‚çŽ‡
        max_response_length = kwargs.get("max_new_tokens", 1024)
        truncate_length = self.prompt_max_len + max_response_length
        # truncate_lengthï¼šæ€»åºåˆ—æœ€å¤§é•¿åº¦ä¸Šé™ = prompt æœ€å¤§é•¿åº¦ + å›žå¤æœ€å¤§é•¿åº¦
        # ç”¨äºŽåŽç»­å°†æ‹¼æŽ¥åŽçš„åºåˆ—ç»Ÿä¸€è£å‰ªï¼Œé¿å…è¿‡é•¿å¯¼è‡´æ˜¾å­˜æˆ–åŽå¤„ç†å¼€é”€è¿‡å¤§

        # Expand prompt list based on the number of samples per prompt
        n_samples_per_prompt = kwargs.pop("n_samples_per_prompt", args.n_samples_per_prompt)
        all_prompts = sum([[prompt] * n_samples_per_prompt for prompt in all_prompts], [])
        all_labels = sum([[label] * n_samples_per_prompt for label in all_labels], [])
        all_prompt_token_ids = self.tokenize_fn(all_prompts, self.prompt_max_len, padding=False)["input_ids"]
        # è¿™é‡Œä¼šæŠŠæ¯æ¡ prompt å¤åˆ¶ n æ¬¡ï¼ˆåšå¤šæ ·æ€§é‡‡æ ·ï¼‰ï¼Œå¹¶åœ¨ä¸ padding çš„å‰æä¸‹æ‹¿åˆ°æ¯æ¡ prompt çš„ token åºåˆ—
        # æ³¨æ„ï¼šä¸ padding å¯ä»¥å‡å°‘æ— æ•ˆè®¡ç®—ï¼Œä¾¿äºŽæŠŠ prompt æŒ‰é•¿åº¦åˆ†å‘åˆ°ä¸åŒå¼•æ“Ž

        # Distribute requests to engines and collect responses
        refs = []
        batch_size = (len(all_prompt_token_ids) + len(llms) - 1) // len(llms)
        for i, llm in enumerate(llms):
            prompt_token_ids = all_prompt_token_ids[i * batch_size : (i + 1) * batch_size]
            refs.append(llm.add_requests.remote(sampling_params=sampling_params, prompt_token_ids=prompt_token_ids))
        ray.get(refs)
        # å°†è¯·æ±‚æŒ‰è¿‘ä¼¼å‡åŒ€çš„æ‰¹å¤§å°åˆ‡åˆ†å¹¶åˆ†å‘ç»™å„ä¸ª vLLM å¼•æ“Žï¼›
        # ray.get(refs) ç”¨äºŽç¡®ä¿æ‰€æœ‰è¯·æ±‚å·²æˆåŠŸæäº¤ï¼ˆè€Œéžç­‰å¾…ç”Ÿæˆå®Œæˆï¼‰ã€‚

        # Retrieve and combine results from all outputs
        all_output_refs = []
        for i, llm in enumerate(llms):
            all_output_refs.append(llm.get_responses.remote())
        print("get responses")
        # æ”¶é›†æ‰€æœ‰ vLLM å¼•æ“Žçš„å“åº”ç»“æžœ
        # all_outputs æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ª output å¯¹è±¡
        # output å¯¹è±¡åŒ…å« prompt_token_ids å’Œ outputs åˆ—è¡¨
        all_outputs = sum(ray.get(all_output_refs), [])
        # è¿™é‡Œå†ç»Ÿä¸€æ”¶é›†å„å¼•æ“Žè¿”å›žçš„ç”Ÿæˆç»“æžœï¼Œå¹¶æŒ‰é¡ºåºåˆå¹¶ä¸ºä¸€ä¸ªåˆ—è¡¨ã€‚

        # Process outputs into Experience objects
        # éåŽ†æ¯ä¸ª vLLM å¼•æ“Žè¿”å›žçš„ outputï¼Œå°†å…¶è½¬æ¢ä¸º Experience å¯¹è±¡
        samples_list = []
        for i in range(len(all_outputs)):
            # all_outputs[i] æ˜¯ä¸€ä¸ª output å¯¹è±¡ï¼ŒåŒ…å«ï¼š
            # - prompt_token_ids: è¾“å…¥çš„ prompt çš„ token IDs
            # - outputs: åŒ…å«ç”Ÿæˆå“åº”çš„åˆ—è¡¨ï¼ˆé€šå¸¸åªæœ‰ä¸€ä¸ªå…ƒç´ ï¼‰
            output = all_outputs[i]
            prompt = all_prompts[i]
            label = all_labels[i]
            # å°†ç¬¬ i ä¸ªè¾“å‡ºä¸Žå¯¹åº”çš„ prompt/label å¯¹é½ï¼Œä¿è¯åŽç»­ç›‘ç£ä¿¡å·ä¸Žé‡‡æ ·ç»“æžœä¸€ä¸€å¯¹åº”ã€‚
            
            # output ç»“æž„è¯´æ˜Žï¼ˆæ¥è‡ª vLLM å¼•æ“Žè¿”å›žï¼‰ï¼š
            # output = {
            #     'prompt_token_ids': [1, 2, 3, ...],  # è¾“å…¥çš„ prompt çš„ token IDs
            #     'outputs': [                          # è¿™æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«ç”Ÿæˆçš„å“åº”
            #         {
            #             'token_ids': [100, 101, 102, ...],  # ç”Ÿæˆçš„ response çš„ token IDs
            #             'logprobs': [                        # æ¯ä¸ªç”Ÿæˆä½ç½®çš„æ¦‚çŽ‡åˆ†å¸ƒå­—å…¸åˆ—è¡¨
            #                 {100: LogprobInfo(logprob=-2.1), 101: LogprobInfo(logprob=-1.8), ...},  # ç¬¬0ä¸ªä½ç½®
            #                 {200: LogprobInfo(logprob=-1.5), 201: LogprobInfo(logprob=-2.3), ...},  # ç¬¬1ä¸ªä½ç½®
            #                 # ... æ¯ä¸ªä½ç½®åŒ…å«æ‰€æœ‰å¯èƒ½tokençš„æ¦‚çŽ‡åˆ†å¸ƒ
            #             ]
            #             # å…¶ä»–å¯èƒ½çš„å­—æ®µ...
            #         }
            #         # ç†è®ºä¸Šå¯èƒ½æœ‰å¤šä¸ªè¾“å‡ºï¼Œä½†é€šå¸¸åªæœ‰ä¸€ä¸ª
            #     ]
            # }
            # æ³¨æ„ï¼šoutputs[0] è¡¨ç¤ºç¬¬ä¸€ä¸ªï¼ˆé€šå¸¸ä¹Ÿæ˜¯å”¯ä¸€çš„ï¼‰ç”Ÿæˆå“åº”

            # Concatenate prompt and output tokens
            # output.outputs[0] èŽ·å–ç¬¬ä¸€ä¸ªç”Ÿæˆå“åº”ï¼ˆé€šå¸¸ä¹Ÿæ˜¯å”¯ä¸€çš„ï¼‰
            # output.outputs[0].token_ids åŒ…å«ç”Ÿæˆçš„ response çš„ token IDs
            input_ids = list(output.prompt_token_ids) + list(output.outputs[0].token_ids)   #æ‹¼æŽ¥è¾“å…¥å’Œè¾“å‡ºåºåˆ—

            attention_mask = [1] * len(input_ids)
            # attention_maskï¼šä¸Ž input_ids ç­‰é•¿ï¼Œ1 è¡¨ç¤ºæœ‰æ•ˆ tokenï¼Œ0 è¡¨ç¤º paddingï¼›è¿™é‡Œæ—  paddingï¼Œæ•…å…¨ä¸º 1ã€‚

            sequences = torch.tensor(input_ids)

            attention_mask = torch.tensor(attention_mask)

            # Create action mask based on output token positions
            action_mask = torch.zeros_like(attention_mask)
            # èŽ·å–ç”Ÿæˆå“åº”çš„é•¿åº¦ï¼Œç”¨äºŽåŽç»­åˆ›å»º action_mask
            response_length = len(output.outputs[0].token_ids)
            # å°† response éƒ¨åˆ†ï¼ˆä»Ž prompt ç»“æŸä½ç½®å¼€å§‹ï¼‰çš„ mask è®¾ä¸º 1ï¼Œprompt éƒ¨åˆ†ä¿æŒä¸º 0
            action_mask[len(output.prompt_token_ids) : len(output.prompt_token_ids) + response_length] = 1   #æŠŠresponseéƒ¨åˆ†maskä¸º1ï¼Œpromptä¸º0
            # action_maskï¼šç”¨äºŽåŒºåˆ†ã€ŒåŠ¨ä½œã€(response) ä¸Žã€Œä¸Šä¸‹æ–‡ã€(prompt) çš„ä½ç½®ï¼Œ
            # åªæœ‰ response éƒ¨åˆ†ä¸º 1ï¼Œä¼šå‚ä¸Žç­–ç•¥æ¢¯åº¦/ä¼˜åŠ¿è®¡ç®—ï¼Œprompt éƒ¨åˆ†ä¸º 0ã€‚

            # Calculate rollout log probs
            rollout_log_probs = None
            if self.strategy.args.enable_vllm_is_correction:   # é»˜è®¤ä¸ä½¿ç”¨
                rollout_log_probs = []
                # èŽ·å–ç”Ÿæˆå“åº”çš„ token IDsï¼Œç”¨äºŽåŽç»­è®¡ç®—å¯¹æ•°æ¦‚çŽ‡
                response_ids = list(output.outputs[0].token_ids)
                # éåŽ†æ¯ä¸ªç”Ÿæˆ token çš„å¯¹æ•°æ¦‚çŽ‡
                # output.outputs[0].logprobs æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«è¯¥ä½ç½®æ‰€æœ‰å¯èƒ½ token çš„æ¦‚çŽ‡åˆ†å¸ƒ
                for i, logprob in enumerate(output.outputs[0].logprobs):
                    # ä»Žæ¦‚çŽ‡åˆ†å¸ƒä¸­æå–å®žé™…ç”Ÿæˆ token çš„å¯¹æ•°æ¦‚çŽ‡
                    rollout_log_probs.append(logprob[response_ids[i]].logprob)

                rollout_log_probs = torch.tensor([0.0] * len(list(output.prompt_token_ids)) + rollout_log_probs)
                rollout_log_probs = rollout_log_probs[1:truncate_length].to("cpu")
                # å½“å¯ç”¨ IS-correctionï¼š
                # - vLLM ä¼šè¿”å›žæ¯ä¸ªç”Ÿæˆ token çš„å¯¹æ•°æ¦‚çŽ‡ï¼Œè¿™é‡Œå°†å…¶ä¸Ž prompt çš„é•¿åº¦å¯¹é½ï¼ˆåœ¨å‰é¢è¡¥é›¶ï¼‰
                # - ç„¶åŽè£å‰ªåˆ° truncate_lengthï¼Œå¹¶æ”¾åˆ° CPUï¼Œä¾¿äºŽåŽç»­ç»éªŒæ‹¼æŽ¥ä¸ŽèŠ‚çœæ˜¾å­˜

            sequences = sequences[:truncate_length].to("cpu")
            attention_mask = attention_mask[:truncate_length].to("cpu")
            action_mask = action_mask[1:truncate_length].to("cpu")
            total_length = attention_mask.float().sum()
            is_clipped = response_length >= max_response_length
            # æ³¨æ„ï¼šaction_mask ä»Žç´¢å¼• 1 å¼€å§‹è£å‰ªï¼Œä»¥ä¸ŽåŽç»­å³ç§»å¯¹é½çš„è¡Œä¸ºä¿æŒä¸€è‡´ï¼ˆå¸¸è§äºŽè‡ªå›žå½’è®­ç»ƒåœºæ™¯ï¼‰ã€‚
            # total_lengthï¼šå½“å‰åºåˆ—æ€» token æ•°ï¼ˆç”¨äºŽåŠ¨æ€ batch åˆ‡åˆ†ç­‰ï¼‰ï¼Œis_clippedï¼šå›žå¤æ˜¯å¦å‘½ä¸­æœ€å¤§é•¿åº¦ä¸Šé™ã€‚

            info = {
                "response_length": torch.tensor([response_length]),
                "total_length": torch.tensor([total_length]),
                "response_clip_ratio": torch.tensor([is_clipped]),
            }
            # info ä¸­è®°å½•æ¯æ¡æ ·æœ¬çš„ç»Ÿè®¡é‡ï¼š
            # - response_lengthï¼šå›žå¤ token æ•°
            # - total_lengthï¼šæ€»åºåˆ—é•¿åº¦ï¼ˆprompt+responseï¼‰
            # - response_clip_ratioï¼šæ˜¯å¦å‘ç”Ÿäº†æœ€å¤§é•¿åº¦è£å‰ªï¼ˆå¸ƒå°”å€¼ï¼‰

            rollout_samples = Experience(
                sequences=sequences.unsqueeze(0),      #å®Œæ•´çš„tokenåºåˆ—
                attention_mask=attention_mask.unsqueeze(0),   #æŽ©ç 
                action_mask=action_mask.unsqueeze(0),     #åŒºåˆ†åŠ¨ä½œ(response)å’Œprompt
                rollout_log_probs=rollout_log_probs.unsqueeze(0) if rollout_log_probs is not None else None,
                prompts=[prompt],
                labels=[label],
                info=info,
            )
            samples_list.append(rollout_samples)
            # æ¯ä¸ª output æž„é€ æˆä¸€ä¸ª Experienceï¼ˆæ‰¹ç»´åº¦ä¸º 1ï¼‰ï¼š
            # - sequences/attention_mask/action_maskï¼šå¼ é‡å½¢çŠ¶ç»Ÿä¸€
            # - prompts/labelsï¼šä»¥åˆ—è¡¨å½¢å¼ä¿å­˜åŽŸæ–‡æœ¬ä¸Žæ ‡ç­¾
            # - rollout_log_probsï¼šä»…åœ¨å¯ç”¨ IS-correction æ—¶å¯ç”¨

        # Get rewards from remote reward models if needed
        # This is required by dynamic sampling
        remote_reward_model = kwargs.get("remote_reward_model", None)
        print(remote_reward_model)
        if remote_reward_model:
            print("get rewards")
            all_queries = sum(
                [
                    self.tokenizer.batch_decode(
                        remove_pad_token(s.sequences, s.action_mask), skip_special_tokens=False   #åŽŸå…ˆæ˜¯attention_mask
                    )
                    for s in samples_list
                ],
                [],
            )   
            all_prompts = sum([s.prompts for s in samples_list], [])
            all_labels = sum([s.labels for s in samples_list], [])
            # å¦‚æžœä¼ å…¥äº†è¿œç«¯å¥–åŠ±æ¨¡åž‹ï¼ˆå¦‚çŽ¯å¢ƒã€HTTP æœåŠ¡æˆ–è‡ªå®šä¹‰å‡½æ•°ï¼‰ï¼Œ
            # æˆ‘ä»¬éœ€è¦æŠŠå½“å‰ç”Ÿæˆçš„ queryï¼ˆprompt+responseï¼‰ä¸Ž prompt/label ä¸€å¹¶å‘é€ä»¥èŽ·å–å¥–åŠ±ä¿¡æ¯ã€‚
            # è¿™é‡Œ decode æ—¶é€‰æ‹© skip_special_tokens=Falseï¼Œç¡®ä¿ç”¨äºŽå¥–åŠ±æ¨¡åž‹çš„è¾“å…¥æ›´è´´è¿‘çœŸå®žæ¨¡åž‹è¾“å‡ºã€‚

            # Get rewards info from remote model
            rewards_info = ray.get(remote_reward_model.get_rewards.remote(all_queries, all_prompts, all_labels))
            # Process rewards and scores
            update_samples_with_rewards(rewards_info, samples_list)
            # å°†è¿œç«¯è¿”å›žçš„å¥–åŠ±/è¯„åˆ†/é¢å¤–æ—¥å¿—å†™å›žåˆ°æ¯æ¡ Experienceï¼Œä¾›åŽç»­ä¼˜åŠ¿ä¼°è®¡ä¸Žæ—¥å¿—è®°å½•ä½¿ç”¨ã€‚
        else:
            print("no rewards in rollout")

        return samples_list


class RemoteExperienceMaker(ABC):
    def __init__(
        self,
        actor_model_group: RayActorGroup,
        critic_model_group: RayActorGroup,
        reward_model_group: RayActorGroup,
        initial_model_group: RayActorGroup,
        kl_controller,
        strategy=None,
        tokenizer=None,
        remote_reward_model=None,
        **kwargs,
    ):
        super().__init__()

        self.actor_model_group = actor_model_group
        self.critic_model_group = critic_model_group
        self.reward_model_group = reward_model_group
        self.initial_model_group = initial_model_group
        self.kl_ctl = kl_controller
        self.strategy = strategy
        self.advantage_estimator = strategy.args.advantage_estimator
        self.args = strategy.args

        # remote_rm_url indicates that the remote reward model is agent enviroment, remote http server or custom reward func
        self.remote_rm_url = self.args.remote_rm_url
        self.remote_reward_model = remote_reward_model
        self.tokenizer = tokenizer

    def split_rollout_samples(self, rollout_samples):
        for i, sample in enumerate(rollout_samples):
            sample.index = [i]    # æ ‡è®°æ¯ä¸ªsampleçš„ä½ç½®

        samples_list = []
        if self.args.use_dynamic_batch:
            total_lengths = [int(s.info["total_length"].item()) for s in rollout_samples]
            effective_actor_num = (
                self.args.actor_num_nodes
                * self.args.actor_num_gpus_per_node
                // self.args.ring_attn_size
                // self.args.ds_tensor_parallel_size
            )
            minimum_batch_num = get_minimum_num_micro_batch_size(
                total_lengths,
                self.args.rollout_max_tokens_per_gpu,
                self.args.ring_attn_size,
                self.args.ds_tensor_parallel_size,
            )
            minimum_batch_num = minimum_batch_num // effective_actor_num * effective_actor_num
            num_batch = max(minimum_batch_num, effective_actor_num)
            batch_indexes = get_seqlen_balanced_partitions(total_lengths, num_batch, False)
            for micro_index in batch_indexes:
                micro_batch = [rollout_samples[idx] for idx in micro_index]
                concat_samples = Experience.concat_experiences(micro_batch, self.tokenizer.pad_token_id)
                samples_list.append(concat_samples)
        else:
            batch_size = self.args.micro_rollout_batch_size     # æŒ‰ç…§ micro_rollout_batch_size çš„å¤§å°åˆ†ç»„
            for i in range(0, len(rollout_samples), batch_size):
                concat_samples = Experience.concat_experiences(
                    rollout_samples[i : i + batch_size], self.tokenizer.pad_token_id
                )
                samples_list.append(concat_samples)
        return samples_list

    @torch.no_grad()
    def make_experience_batch(self, rollout_samples) -> List[Experience]:
        """
        Make a list of experience with the micro_rollout_batch_size.

        This method will first calculate the response sequences and rewards for the given prompts.
        Then, if we need certain processing for the rewards or do certain filtering, we can process the rollout as a whole.
        After that, we will calculate the advantages and returns for each experience.
        """
        # Each batch of samples will be scheduled to a effective Ray Actor (i.e, a DP rank)
        samples_list = self.split_rollout_samples(rollout_samples)   # listé‡Œ micro_rollout_batch_sizeä¸ªexperienceæ‰“åŒ…æˆä¸€ä¸ªexperience
        print("finish split samples")
        print(f"sample listé‡Œçš„æ ·æœ¬æ•°é‡æ˜¯{len(samples_list)}")
        print(samples_list[0])
        # Make experiences (models forward: logprobs, values, rewards, and kl divergence)
        experiences = self.make_experience(samples_list)

        # Process experiences (reward shaping, etc.)
        experiences = self.compute_advantages_and_returns(experiences)
        return experiences

    @torch.no_grad()
    def make_experience(self, samples_list: List[Experience]) -> List[Experience]:
        """
        Turn samples into experience by calculating logprobs, values, rewards, and kl divergence.
        """
        start_time = time.time()
        logger.info(f"ðŸš€ Starting experience making with {sum([len(s.sequences) for s in samples_list])} samples")

        args = self.strategy.args
        device = "cpu"

        # Extract all information from samples in one pass
        # Convert samples into lists of tensors and metadata for batch processing
        sequences_list = [s.sequences for s in samples_list]
        attention_mask_list = [s.attention_mask for s in samples_list]
        action_mask_list = [s.action_mask for s in samples_list] 
        reward_action = []  # æ¢å¤ä¸ºå‡å°‘å‰çš„ action_maskï¼ˆåœ¨å‰é¢è¡¥ä¸€ä¸ª 0ï¼‰ï¼Œè¾“å‡ºå½¢çŠ¶ç»Ÿä¸€ä¸º [bs, L+1]
        for am in action_mask_list:
            # è§„èŒƒå½¢çŠ¶ä¸º [bs, L]
            print(f"amçš„å½¢çŠ¶æ˜¯{am.shape}")
            if am.dim() == 1:
                am_2d = am.unsqueeze(0)
            elif am.dim() == 2:
                am_2d = am
            else:
                am_2d = am.view(am.size(0), -1)

            bs = am_2d.size(0)
            leading_zero_col = torch.zeros(bs, 1, dtype=am_2d.dtype, device=am_2d.device)
            full_mask_2d = torch.cat([leading_zero_col, am_2d], dim=1)
            print(f"full_mask_2dçš„å½¢çŠ¶æ˜¯{full_mask_2d.shape}")
            reward_action.append(full_mask_2d)

        # The rewards are already filled in the samples_list, such as the agent's environment rewards
        if samples_list[0].rewards is not None:
            pass
        elif self.remote_rm_url:
            print("no rewards and now get")
            queries_list = sum(
                [
                    self.tokenizer.batch_decode(remove_pad_token(seq, action_mask), skip_special_tokens=False)
                    for seq, action_mask in zip(sequences_list, reward_action)
                ],
                [],
            )
            # print(queries_list[0])
            print("finish decode response for rewards")
            prompts_list = sum([s.prompts for s in samples_list], [])
            labels_list = sum([s.labels for s in samples_list], [])
            # Keep the remote call asynchronous
            r_refs = self.remote_reward_model.get_rewards.remote(queries_list, prompts_list, labels_list)
            print("wait for getting rewards")
        else:
            # Batch call reward model
            r_refs = self.reward_model_group.async_run_method_batch(
                method_name="forward",
                sequences=sequences_list,
                attention_mask=attention_mask_list,
                pad_sequence=[True] * len(samples_list),
            )

        # Sync to avoid GPU OOM when colocate models
        if args.colocate_all_models and not self.remote_rm_url:
            ray.get(r_refs)
            ray.get(self.reward_model_group.async_run_method(method_name="empty_cache"))

        # Batch call actor model
        action_log_probs_ref = self.actor_model_group.async_run_method_batch(
            method_name="forward",
            sequences=sequences_list,
            action_mask=action_mask_list,
            attention_mask=attention_mask_list,
        )     # oldç­–ç•¥è¾“å‡ºçš„æ¦‚çŽ‡
        print("wait for getting old logit_probs")

        # Sync to avoid GPU OOM when colocate models
        if args.colocate_all_models or args.colocate_actor_ref:
            ray.get(action_log_probs_ref)
            ray.get(self.actor_model_group.async_run_method(method_name="empty_cache"))

        # Batch call critic model
        if self.critic_model_group is not None:
            if args.colocate_critic_reward and not self.remote_rm_url:
                ray.get(r_refs)
                ray.get(self.reward_model_group.async_run_method(method_name="empty_cache"))

            value_ref = self.critic_model_group.async_run_method_batch(
                method_name="forward",
                sequences=sequences_list,
                action_mask=action_mask_list,
                attention_mask=attention_mask_list,
            )
            if args.colocate_all_models or args.colocate_critic_reward:
                ray.get(value_ref)
                ray.get(self.critic_model_group.async_run_method(method_name="empty_cache"))
        else:
            value_ref = ray.put([[None]] * (len(samples_list) * args.ring_attn_size * args.ds_tensor_parallel_size))

        # Batch call initial model
        if self.initial_model_group is not None:
            base_action_log_probs_ref = self.initial_model_group.async_run_method_batch(
                method_name="forward",
                sequences=sequences_list,
                action_mask=action_mask_list,
                attention_mask=attention_mask_list,
            )     # refç­–ç•¥è¾“å‡ºçš„æ¦‚çŽ‡
            print("wait for getting ref logit_probs")

            if args.colocate_all_models or args.colocate_actor_ref:
                ray.get(base_action_log_probs_ref)
                ray.get(self.initial_model_group.async_run_method(method_name="empty_cache"))
        else:
            base_action_log_probs_ref = ray.put(
                [[None]] * (len(samples_list) * args.ring_attn_size * args.ds_tensor_parallel_size)
            )

        # Wait for all remote calls to complete and flatten the results
        # Note: the results duplicated ring_attn_size * ds_tensor_parallel_size times
        # This is because the actors in ring group and tp group will return the same output
        duplicate_factor = args.ring_attn_size * args.ds_tensor_parallel_size
        print("get old logit_probs")
        action_log_probs_list = sum(ray.get(action_log_probs_ref)[::duplicate_factor], [])
        print("get ref logit_probs")
        base_action_log_probs_list = sum(ray.get(base_action_log_probs_ref)[::duplicate_factor], [])
        print("finish logit_probs")
        value_list = sum(ray.get(value_ref)[::duplicate_factor], [])

        # Process rewards based on source
        if samples_list[0].rewards is not None:
            pass
        elif self.remote_rm_url:
            # Get rewards info from remote model
            print("get rewards now")
            rewards_info = ray.get(r_refs)
            # Process rewards and scores
            print("update rewards")
            update_samples_with_rewards(rewards_info, samples_list)
        else:
            # Reward Model
            rewards_list = sum(ray.get(r_refs)[::duplicate_factor], [])
            for i, samples in enumerate(samples_list):
                samples.rewards = rewards_list[i]
                samples.info["reward"] = rewards_list[i]

        assert (
            len(samples_list) == len(action_log_probs_list) == len(base_action_log_probs_list) == len(value_list)
        ), f"len(samples_list): {len(samples_list)}, len(action_log_probs_list): {len(action_log_probs_list)}, len(base_action_log_probs_list): {len(base_action_log_probs_list)}, len(value_list): {len(value_list)}"

        # Process results for each sample
        for i, (samples, action_log_probs, base_action_log_probs, value) in enumerate(
            zip(samples_list, action_log_probs_list, base_action_log_probs_list, value_list)
        ):
            if (self.initial_model_group is not None) and (not args.use_kl_loss):
                kl = compute_approx_kl(
                    action_log_probs,
                    base_action_log_probs,
                    kl_estimator=self.strategy.args.kl_estimator,
                )
            else:
                kl = torch.zeros_like(action_log_probs, dtype=action_log_probs.dtype, device=device)
            kl_mean = masked_mean(kl, samples.action_mask, dim=-1)

            if not args.use_kl_loss:
                base_action_log_probs = None

            # Update experience with new information
            samples.action_log_probs = action_log_probs    # oldçš„action_log_probs
            samples.base_action_log_probs = base_action_log_probs     
            samples.values = value
            samples.kl = kl
            samples.info["kl"] = kl_mean

        end_time = time.time()
        duration = end_time - start_time
        time_str = str(timedelta(seconds=duration)).split(".")[0]
        logger.info(f"âœ¨ Experience making completed in {time_str}")
        return samples_list

    @torch.no_grad()
    def compute_advantages_and_returns(
        self, experiences: List[Experience], **kwargs
    ) -> Tuple[List[Experience], List[torch.Tensor]]:
        """
        Process experiences, this can be used to filter out some experiences or do some processing on the rewards.
        Example, use_dynamic_batch
            >>> rewards: [0, 1, 0.5, 1], indices: [1, 2, 0, 3], n_samples_per_prompt: 2
            >>> sorted rewards: [0,5, 0, 1, 1], reward shaping: [0.25, 0.25, 1, 1]
            >>> map back: [0.25, 1, 0.25, 1]
        Output:
        - experiences: List of Experience
        - rewards: List of rewards
        """
        args = self.strategy.args

        # DAPO reward shaping with optional overlong penalty - Apply BEFORE dynamic indices processing
        if args.overlong_buffer_len is not None:
            assert (
                args.generate_max_len >= args.overlong_buffer_len
            ), "generate_max_len must be larger than overlong_buffer_len"
            overlong_buffer_len = args.overlong_buffer_len
            expected_len = args.generate_max_len - overlong_buffer_len
            overlong_penalty_factor = args.overlong_penalty_factor

            # Apply penalty to each experience's rewards based on response length
            for experience in experiences:
                response_lengths = experience.info["response_length"]
                batch_size = len(response_lengths)
                for j in range(batch_size):
                    valid_response_length = response_lengths[j].item()
                    # Cap the exceed_len to overlong_buffer_len to prevent excessive penalty
                    exceed_len = min(valid_response_length - expected_len, overlong_buffer_len)
                    if exceed_len > 0:
                        overlong_penalty = -exceed_len / overlong_buffer_len * overlong_penalty_factor
                        # Apply penalty to the j-th reward in this experience
                        experience.rewards[j] += overlong_penalty

        # èŽ·å–æ¯ä¸ª experience ä¸­æ ·æœ¬çš„æ•°é‡ï¼ˆç”¨äºŽåŽç»­åˆ†å‰²ï¼‰
        # exp_len[i] è¡¨ç¤ºç¬¬ i ä¸ª experience åŒ…å«å¤šå°‘ä¸ªæ ·æœ¬
        exp_len = [len(experience.index) for experience in experiences]    
        
        # æž„å»ºåŽŸå§‹ä½ç½®ç´¢å¼•æ˜ å°„
        # indices çš„ä½œç”¨ï¼š
        # - å½“ä¸ä½¿ç”¨åŠ¨æ€æ‰¹å¤„ç†æ—¶ï¼šindices = [0, 1, 2, 3, ...]ï¼ˆèº«ä»½æ˜ å°„ï¼‰
        # - å½“ä½¿ç”¨åŠ¨æ€æ‰¹å¤„ç†æ—¶ï¼šindices è®°å½•äº†æ¯ä¸ªæ ·æœ¬åœ¨åŽŸå§‹ rollout_samples ä¸­çš„ä½ç½®
        # ä¾‹å¦‚ï¼šå¦‚æžœæ ·æœ¬è¢«é‡æ–°æŽ’åˆ—ï¼Œindices å¯èƒ½æ˜¯ [2, 0, 4, 1, 3, ...]
        indices = torch.tensor(sum([experience.index for experience in experiences], []))
        
        # å°†æ‰€æœ‰ experience çš„å¥–åŠ±æ‹¼æŽ¥æˆä¸€ä¸ªå¤§çš„å¼ é‡
        # raw_rewards çš„å½¢çŠ¶ï¼š(total_samples,)ï¼ŒåŒ…å«æ‰€æœ‰æ ·æœ¬çš„åŽŸå§‹å¥–åŠ±
        # æ³¨æ„ï¼šè¿™é‡Œçš„é¡ºåºå¯èƒ½ä¸ŽåŽŸå§‹ç”Ÿæˆé¡ºåºä¸åŒï¼ˆå¦‚æžœä½¿ç”¨äº†åŠ¨æ€æ‰¹å¤„ç†ï¼‰
        raw_rewards = torch.cat([experience.rewards for experience in experiences], dim=0)
         
        # åˆ›å»ºä¸€ä¸ªç©ºçš„å¥–åŠ±å¼ é‡ï¼Œç”¨äºŽå­˜å‚¨é‡æŽ’åºåŽçš„å¥–åŠ±
        rewards = torch.empty_like(raw_rewards)
        
        # ä½¿ç”¨ indices å°†åŽŸå§‹å¥–åŠ±é‡æ–°æŽ’åºåˆ°æ­£ç¡®ä½ç½®
        # rewards[indices] = raw_rewards çš„å«ä¹‰ï¼š
        # - å°† raw_rewards[0] æ”¾åˆ° rewards[indices[0]] ä½ç½®
        # - å°† raw_rewards[1] æ”¾åˆ° rewards[indices[1]] ä½ç½®
        # - ä»¥æ­¤ç±»æŽ¨ï¼Œæ¢å¤åŽŸå§‹çš„é¡ºåº
        rewards[indices] = raw_rewards  # sorted
        
        # å°†ä¸€ç»´å¥–åŠ±é‡å¡‘ä¸ºäºŒç»´ï¼Œä¾¿äºŽç»„å†…å¤„ç†
        # é‡å¡‘åŽçš„å½¢çŠ¶ï¼š(num_prompts, n_samples_per_prompt)
        # ä¾‹å¦‚ï¼šå¦‚æžœæœ‰ 3 ä¸ª promptï¼Œæ¯ä¸ª prompt ç”Ÿæˆ 2 ä¸ªæ ·æœ¬ï¼Œåˆ™å½¢çŠ¶ä¸º (3, 2)
        # è¿™æ ·ä¾¿äºŽåŽç»­è¿›è¡Œ GRPO çš„ç»„å†…å¥–åŠ±æ•´å½¢ï¼ˆå¦‚å‡åŽ»ç»„å†…å‡å€¼ã€è®¡ç®—ç»„å†…æ ‡å‡†å·®ç­‰ï¼‰
        rewards = rewards.reshape(-1, args.n_samples_per_prompt)

        # log group reward std
        if args.n_samples_per_prompt > 1:
            group_reward_stds = (
                rewards.std(-1, keepdim=True).repeat(1, args.n_samples_per_prompt).reshape(-1)[indices].split(exp_len)
            )
            for experience, group_reward_std in zip(experiences, group_reward_stds):
                experience.info["group_reward_std"] = group_reward_std

        # reward shaping
        if args.advantage_estimator == "rloo":
            baseline = (rewards.sum(-1, keepdim=True) - rewards) / (args.n_samples_per_prompt - 1)
            rewards = rewards - baseline
        elif args.advantage_estimator in ["reinforce_baseline", "dr_grpo"]:
            # REINFORCE++-baseline and Dr. GRPO removed the `/std` in GRPO as `/ std` is not needed in RL variance reduction theory.
            # And `k3 KL` has a larger variance than `k1 KL` under a categorical distribution.
            rewards = rewards - rewards.mean(-1, keepdim=True)
        elif args.advantage_estimator == "group_norm":
            rewards = (rewards - rewards.mean(-1, keepdim=True)) / (rewards.std(-1, keepdim=True) + 1e-9)

        rewards = rewards.reshape(-1)[indices].split(exp_len)  # å±•å¹³ä¸ºä¸€ç»´ï¼Œç„¶åŽæ ¹æ®exp_lenåˆ†å‰²æˆå…ƒç»„ï¼Œå…ƒç»„ä¸­æœ‰å¤šä¸ª[bs]å¤§å°çš„å¼ é‡ï¼Œæ­¤æ—¶çš„rewardå·²ç»æ˜¯å½’ä¸€åŒ–çš„ä¼˜åŠ¿
        # print(f"æ­¤æ—¶å½’ä¸€åŒ–åŽçš„å¥–åŠ±æ˜¯ï¼š{rewards}")
        # for experience, reward in zip(experiences, rewards):
        #     experience.advantages = reward


        # calculate return and advantages
        for experience, reward in zip(experiences, rewards):
            reward = compute_reward(
                reward,
                self.kl_ctl.value,
                experience.kl,
                action_mask=experience.action_mask,
                reward_clip_range=args.reward_clip_range,
            )
            # print(f"æ­¤æ—¶rewardè®¡ç®—å®Œæ˜¯ï¼š{reward},å½¢çŠ¶æ˜¯{reward.shape}")
            # time.sleep(10)

            if self.advantage_estimator == "gae":
                experience.advantages, experience.returns = self.get_advantages_and_returns(
                    experience.values,
                    reward,
                    experience.action_mask,
                    args.gamma,
                    args.lambd,
                )
            elif self.advantage_estimator in ["reinforce", "rloo", "reinforce_baseline", "group_norm", "dr_grpo"]:
                if args.gamma != 1.0 and self.advantage_estimator in [
                    "rloo",
                    "reinforce_baseline",
                    "group_norm",
                    "dr_grpo",
                ]:
                    logger.warning("gamma is set to 1.0 for rloo, reinforce_baseline, and group_norm")
                    args.gamma = 1.0

                experience.returns = self.get_cumulative_returns(
                    reward,
                    experience.action_mask,
                    args.gamma,
                )
                experience.advantages = deepcopy(experience.returns)
                # print(f"æ­¤æ—¶è¿”å›žæ˜¯ï¼š{experience.returns},å½¢çŠ¶æ˜¯{experience.returns.shape}")
                # print(f"æ­¤æ—¶ä¼˜åŠ¿æ˜¯ï¼š{experience.advantages},å½¢çŠ¶æ˜¯{experience.advantages.shape}")
            else:
                raise Exception(f"Unkown advantage_estimator {self.advantage_estimator}")

            # calculate the return info.
            return_sums = reward.sum(dim=-1)
            experience.info["return"] = return_sums
            # remove unnecessary info
            experience.kl = None

        # Normalize advantages across all experiences for GAE, REINFORCE, and REINFORCE-baseline
        if self.args.advantage_estimator in ["gae", "reinforce", "reinforce_baseline"]:
            all_advantages = []
            all_action_masks = []
            for exp in experiences:
                all_advantages.append(exp.advantages.flatten())
                all_action_masks.append(exp.action_mask.flatten())

            advantages_vector = torch.cat(all_advantages, dim=0).float()
            action_masks_vector = torch.cat(all_action_masks, dim=0)
            num_actions = action_masks_vector.sum()

            # mean
            mean = (advantages_vector * action_masks_vector).sum() / num_actions
            # std
            if not self.args.no_advantage_std_norm:
                var = ((advantages_vector - mean).pow(2) * action_masks_vector).sum() / num_actions
                rstd = var.clamp(min=1e-8).rsqrt()
            else:
                rstd = 1

            # Apply normalization to each experience
            for exp in experiences:
                exp.advantages = (exp.advantages - mean) * rstd

        return experiences

    @torch.no_grad()
    def get_advantages_and_returns(
        self,
        values: torch.Tensor,
        rewards: torch.Tensor,
        action_mask: torch.Tensor,
        gamma: float,
        lambd: float,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Function that computes advantages and returns from rewards and values.
        Calculated as in the original PPO paper: https://arxiv.org/abs/1707.06347
        Note that rewards may include a KL divergence loss term.

        Advantages looks like this:
        Adv1 =  R1 + Î³ * Î» * R2     + Î³^2 * Î»^2 * R3       + ...
              - V1 + Î³ * (1 - Î») V2 + Î³^2 * Î» * (1 - Î») V3 + ...

        Returns looks like this:
        Ret1 =  R1 + Î³ * Î» * R2     + Î³^2 * Î»^2 * R3       + ...
                   + Î³ * (1 - Î») V2 + Î³^2 * Î» * (1 - Î») V3 + ...

        Input:
        - values: Tensor of shape (batch_size, response_size)
        - rewards: Tensor of shape (batch_size, response_size)

        Output:
        - advantages: Tensor of shape (batch_size, response_size)
        - returns: Tensor of shape (batch_size, response_size)
        """
        lastgaelam = 0
        advantages_reversed = []
        response_length = rewards.size(1)

        # Mask invalid responses
        if action_mask is not None:
            values = action_mask * values
            rewards = action_mask * rewards

        for t in reversed(range(response_length)):
            nextvalues = values[:, t + 1] if t < response_length - 1 else 0.0
            delta = rewards[:, t] + gamma * nextvalues - values[:, t]
            lastgaelam = delta + gamma * lambd * lastgaelam
            advantages_reversed.append(lastgaelam)
        advantages = torch.stack(advantages_reversed[::-1], dim=1)
        returns = advantages + values
        return advantages.detach(), returns

    @torch.no_grad()
    def get_cumulative_returns(
        self,
        rewards: torch.Tensor,
        action_mask: torch.Tensor,
        gamma: float,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Function that computes advantages and returns from rewards using REINFORCE.
        REINFORCE uses cumulative returns without the GAE (Generalized Advantage Estimation).

        Input:
        - rewards: Tensor of shape (batch_size, response_size)
        - action_mask: Tensor of shape (batch_size, response_size), binary mask
        - gamma: discount factor

        Output:
        - returns: Tensor of shape (batch_size, response_size)
        """
        response_length = rewards.size(1)
        returns = torch.zeros_like(rewards)
        cumulative_return = torch.zeros(rewards.size(0), device=rewards.device)

        # Mask invalid responses if action_mask is provided
        if action_mask is not None:
            rewards = action_mask * rewards

        # Calculate returns by accumulating discounted rewards
        for t in reversed(range(response_length)):
            cumulative_return = rewards[:, t] + gamma * cumulative_return
            returns[:, t] = cumulative_return

        return returns
